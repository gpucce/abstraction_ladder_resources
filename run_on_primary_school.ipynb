{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b4f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.utils import tokenize_and_align_labels\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = \"with_spec\"\n",
    "experiment_name = \"without_spec\"\n",
    "# experiment_name = \"without_spec_t1_vs_t2\"\n",
    "\n",
    "if experiment_name == \"with_spec\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_data_all_with_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_data_all_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec_t1_vs_t2\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t1_test_t2_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec_t2_vs_t1\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t2_test_t1_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"with_spec_t1_vs_t2\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t1_test_t2_with_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"with_spec_t2_vs_t1\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t2_test_t1_with_spec.ds\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"experiment_name must be one of the predefined options.\")\n",
    "\n",
    "run_path = \"./multirun/2025-07-24/18-18-26\"\n",
    "\n",
    "model_name_or_path = None\n",
    "runs = os.listdir(run_path)\n",
    "for run in runs:\n",
    "    with open(os.path.join(run_path, run, \".hydra\", \"config.yaml\")) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    if config[\"args\"][\"dataset_name\"].split('/')[-1] == dataset_name.split('/')[-1]:\n",
    "        model_name_or_path = os.path.join(run_path, run)\n",
    "        break\n",
    "\n",
    "assert model_name_or_path is not None, f\"Model not found for dataset {dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377d03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name_or_path,trust_remote_code=True,)\n",
    "model.to(\"cuda\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48173d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a8ab4ab40a420cb78715e29a623ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_from_disk(dataset_name)\n",
    "ref_ds = load_from_disk(dataset_name)\n",
    "if \"validation\" in ds:\n",
    "    ds = ds[\"validation\"]\n",
    "    ref_ds = ref_ds[\"validation\"]\n",
    "\n",
    "ds = ds.map(lambda x: tokenize_and_align_labels(x, tokenizer, label_to_id=model.config.label2id), batched=True)\n",
    "new_ds = ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe5505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(ds, ref_ds, idx, do_print=False):\n",
    "    results = []\n",
    "    out = model(**{i:j.to(\"cuda\") for i, j in ds[[idx]].items()})\n",
    "    preds = out.logits.cpu().argmax(-1)[0].tolist()[1:]\n",
    "    tokens = ref_ds[idx][\"full_list\"]\n",
    "\n",
    "    # tokenization check\n",
    "    refs = ds[idx][\"input_ids\"][ds[idx][\"attention_mask\"].bool()].tolist()\n",
    "    check = []\n",
    "    for tok in tokens:\n",
    "        check += tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    assert refs[1:-1] == check\n",
    "\n",
    "    count = 0\n",
    "    for tok in tokens:\n",
    "        tokenized = tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "        label = model.config.id2label[preds[count]]\n",
    "        count += len(tokenized)\n",
    "        results.append({\"token\": tok, \"label\": label,})\n",
    "        if do_print:\n",
    "            print(\"Token:\", tok, \"| Label:\", label)\n",
    "\n",
    "    return results  # Remove the first empty token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c11dab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abf943b82594355b5bee255736e3353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = []\n",
    "for idx in tqdm(range(len(ref_ds))):\n",
    "    result = get_results(ds, ref_ds, idx)\n",
    "    all_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93a99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dfce39045940299cf51c97e0a40609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_metrics(all_results, ref_ds):\n",
    "    total = 0\n",
    "    metrics = {\"n\": 0, \"p\": 0, \"tp\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"List Acc\": 0, \"List Acc @1\": 0, \"List Acc @2\": 0}\n",
    "    for idx in tqdm(range(len(ref_ds))):\n",
    "        results = all_results[idx]\n",
    "        full_list = ref_ds[idx][\"full_list\"]\n",
    "        clean_results = ref_ds[idx][\"clean_list\"]\n",
    "        total += len(full_list)\n",
    "        metrics[\"p\"] += len(clean_results)\n",
    "        metrics[\"n\"] += len(full_list) - len(clean_results)\n",
    "        metrics[\"tp\"] += sum(1 for r in results if r[\"label\"] != \"O\" and r[\"token\"] in clean_results)\n",
    "        metrics[\"tn\"] += sum(1 for r in results if r[\"label\"] == \"O\" and r[\"token\"] not in clean_results)\n",
    "        metrics[\"fn\"] += sum(1 for r in results if r[\"label\"] == \"O\" and r[\"token\"] in clean_results)\n",
    "        metrics[\"fp\"] += sum(1 for r in results if r[\"label\"] != \"O\" and r[\"token\"] not in clean_results)\n",
    "        matches = [i[\"token\"] for i in results if i[\"label\"] != \"O\"]\n",
    "        if matches == clean_results:\n",
    "            metrics[\"List Acc\"] += 1\n",
    "        if sum(1 for i in matches if i not in clean_results) + sum(1 for i in clean_results if i not in matches) <= 1:\n",
    "            metrics[\"List Acc @1\"] += 1\n",
    "        if sum(1 for i in matches if i not in clean_results) + sum(1 for i in clean_results if i not in matches) <= 2:\n",
    "            metrics[\"List Acc @2\"] += 1\n",
    "\n",
    "    assert total == (metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"])\n",
    "    assert total == (metrics[\"p\"] + metrics[\"n\"])\n",
    "    assert metrics[\"p\"] == (metrics[\"tp\"] + metrics[\"fn\"])\n",
    "    assert metrics[\"n\"] == (metrics[\"tn\"] + metrics[\"fp\"])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics = get_metrics(all_results, ref_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dc1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Word Level Metrics ====\n",
      "\n",
      "Precision:      0.934\n",
      "Recall:         0.958\n",
      "F1:             0.946\n",
      "Accuracy:       0.926\n",
      "--------------------\n",
      "Positives: 3394 Negatives: 1661 Total: 5055\n",
      "Positive Ratio: 0.671 Negative Ratio: 0.329\n",
      "\n",
      "==== List Level Metrics ====\n",
      "\n",
      "Accuracy:       0.739\n",
      "Accuracy @1:    0.949\n",
      "Accuracy @2:    0.984\n",
      "--------------------\n",
      "Total Lists: 1110\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(metrics):\n",
    "    total = metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"]\n",
    "    print(\"==== Word Level Metrics ====\")\n",
    "    print()\n",
    "    print(\"Precision:\".ljust(15), round(metrics[\"tp\"] / (metrics[\"tp\"] + metrics[\"fp\"]), 3))\n",
    "    print(\"Recall:\".ljust(15), round(metrics[\"tp\"] / (metrics[\"tp\"] + metrics[\"fn\"]), 3))\n",
    "    print(\"F1:\".ljust(15), round(2 * metrics[\"tp\"] / (2 * metrics[\"tp\"] + metrics[\"fp\"] + metrics[\"fn\"]), 3))\n",
    "    print(\"Accuracy:\".ljust(15), round((metrics[\"tp\"] + metrics[\"tn\"]) / (metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"]), 3))\n",
    "    print('-'*20)\n",
    "    print(\"Positives:\", metrics[\"p\"], \"Negatives:\", metrics[\"n\"], \"Total:\", total)\n",
    "    print(\"Positive Ratio:\", round(metrics[\"p\"] / total, 3), \"Negative Ratio:\", round(metrics[\"n\"] / total, 3))\n",
    "    print()\n",
    "    print(\"==== List Level Metrics ====\")\n",
    "    print()\n",
    "    print(\"Accuracy:\".ljust(15), round(metrics[\"List Acc\"] / len(ref_ds), 3))\n",
    "    print(\"Accuracy @1:\".ljust(15), round(metrics[\"List Acc @1\"] / len(ref_ds), 3))\n",
    "    print(\"Accuracy @2:\".ljust(15), round(metrics[\"List Acc @2\"] / len(ref_ds), 3))\n",
    "    print('-'*20)\n",
    "    print(\"Total Lists:\", len(ref_ds))\n",
    "\n",
    "\n",
    "\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08645040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(idx):\n",
    "    print(\"Full List:\".ljust(20), ref_ds[idx][\"full_list\"])\n",
    "    print(\"Cleaned List:\".ljust(20), ref_ds[idx][\"clean_list\"])\n",
    "    predicted_list = []\n",
    "    for i in get_results(ds, ref_ds, idx):\n",
    "        if i[\"label\"] != \"O\":\n",
    "            predicted_list.append(i[\"token\"])\n",
    "    print(\"Predicted List:\".ljust(20), predicted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59cc5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full List:           ['pensiero', 'sogno', 'incubo']\n",
      "Cleaned List:        ['pensiero', 'sogno', 'incubo']\n",
      "Predicted List:      ['pensiero', 'sogno', 'incubo']\n"
     ]
    }
   ],
   "source": [
    "print_results(111)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
