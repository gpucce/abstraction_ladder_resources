{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b4f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.utils import tokenize_and_align_labels\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe6925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = \"with_spec\"\n",
    "experiment_name = \"without_spec\"\n",
    "# experiment_name = \"without_spec_t1_vs_t2\"\n",
    "\n",
    "# run_path = \"/home/gpucce/Repos/abstraction_ladders/acl_abstraction_ladders/primary_school_data/primary_school_experiments/multirun/2025-07-15/18-23-29\"\n",
    "run_path = \"./multirun/2025-07-24/18-18-26\"\n",
    "\n",
    "if experiment_name == \"with_spec\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_data_all_with_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_data_all_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec_t1_vs_t2\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t1_test_t2_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec_t2_vs_t1\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t2_test_t1_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"with_spec_t1_vs_t2\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t1_test_t2_with_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"with_spec_t2_vs_t1\":\n",
    "    dataset_name = \"./src/primary_school/training_datasets/training_t2_test_t1_with_spec.ds\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"experiment_name must be one of the predefined options.\")\n",
    "\n",
    "model_name_or_path = None\n",
    "runs = os.listdir(run_path)\n",
    "for run in runs:\n",
    "    with open(os.path.join(run_path, run, \".hydra\", \"config.yaml\")) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    if config[\"args\"][\"dataset_name\"].split('/')[-1] == dataset_name.split('/')[-1]:\n",
    "        model_name_or_path = os.path.join(run_path, run)\n",
    "        break\n",
    "\n",
    "assert model_name_or_path is not None, f\"Model not found for dataset {dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091cc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_align_labels(examples, tokenizer, label_to_id):\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         examples[\"full_list\"],\n",
    "#         max_length=512,\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "#         is_split_into_words=True,\n",
    "#     )\n",
    "\n",
    "#     labels = []\n",
    "#     for i, label in enumerate(examples[\"label\"]):\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         # print(len(word_ids))\n",
    "#         previous_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids:\n",
    "#             # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "#             # ignored in the loss function.\n",
    "#             if word_idx is None:\n",
    "#                 label_ids.append(-100)\n",
    "#             # We set the label for the first token of each word.\n",
    "#             elif word_idx != previous_word_idx:\n",
    "#                 label_ids.append(label_to_id[label[word_idx]])\n",
    "#             # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "#             # the label_all_tokens flag.\n",
    "#             else:\n",
    "#                 label_ids.append(-100)\n",
    "#             previous_word_idx = word_idx\n",
    "#         # print(\"LABEL IDS\", len(label_ids))\n",
    "#         labels.append(label_ids)\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377d03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name_or_path,trust_remote_code=True,)\n",
    "model.to(\"cuda\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48173d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a8ab4ab40a420cb78715e29a623ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_from_disk(dataset_name)\n",
    "ref_ds = load_from_disk(dataset_name)\n",
    "if \"validation\" in ds:\n",
    "    ds = ds[\"validation\"]\n",
    "    ref_ds = ref_ds[\"validation\"]\n",
    "\n",
    "ds = ds.map(lambda x: tokenize_and_align_labels(x, tokenizer, label_to_id=model.config.label2id), batched=True)\n",
    "new_ds = ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe5505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(ds, ref_ds, idx, do_print=False):\n",
    "    results = []\n",
    "    out = model(**{i:j.to(\"cuda\") for i, j in ds[[idx]].items()})\n",
    "    preds = out.logits.cpu().argmax(-1)[0].tolist()[1:]\n",
    "    tokens = ref_ds[idx][\"full_list\"]\n",
    "\n",
    "    # tokenization check\n",
    "    refs = ds[idx][\"input_ids\"][ds[idx][\"attention_mask\"].bool()].tolist()\n",
    "    check = []\n",
    "    for tok in tokens:\n",
    "        check += tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    assert refs[1:-1] == check\n",
    "\n",
    "    count = 0\n",
    "    for tok in tokens:\n",
    "        tokenized = tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "        label = model.config.id2label[preds[count]]\n",
    "        count += len(tokenized)\n",
    "        results.append({\"token\": tok, \"label\": label,})\n",
    "        if do_print:\n",
    "            print(\"Token:\", tok, \"| Label:\", label)\n",
    "\n",
    "    return results  # Remove the first empty token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c11dab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abf943b82594355b5bee255736e3353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = []\n",
    "for idx in tqdm(range(len(ref_ds))):\n",
    "    result = get_results(ds, ref_ds, idx)\n",
    "    all_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb93a99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dfce39045940299cf51c97e0a40609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total = 0\n",
    "metrics = {\"n\": 0, \"p\": 0, \"tp\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"List Acc\": 0, \"List Acc @1\": 0, \"List Acc @2\": 0}\n",
    "for idx in tqdm(range(len(ref_ds))):\n",
    "    results = all_results[idx]\n",
    "    full_list = ref_ds[idx][\"full_list\"]\n",
    "    clean_results = ref_ds[idx][\"clean_list\"]\n",
    "    total += len(full_list)\n",
    "    metrics[\"p\"] += len(clean_results)\n",
    "    metrics[\"n\"] += len(full_list) - len(clean_results)\n",
    "    metrics[\"tp\"] += sum(1 for r in results if r[\"label\"] != \"O\" and r[\"token\"] in clean_results)\n",
    "    metrics[\"tn\"] += sum(1 for r in results if r[\"label\"] == \"O\" and r[\"token\"] not in clean_results)\n",
    "    metrics[\"fn\"] += sum(1 for r in results if r[\"label\"] == \"O\" and r[\"token\"] in clean_results)\n",
    "    metrics[\"fp\"] += sum(1 for r in results if r[\"label\"] != \"O\" and r[\"token\"] not in clean_results)\n",
    "    matches = [i[\"token\"] for i in results if i[\"label\"] != \"O\"]\n",
    "    if matches == clean_results:\n",
    "        metrics[\"List Acc\"] += 1\n",
    "    if sum(1 for i in matches if i not in clean_results) + sum(1 for i in clean_results if i not in matches) <= 1:\n",
    "        metrics[\"List Acc @1\"] += 1\n",
    "    if sum(1 for i in matches if i not in clean_results) + sum(1 for i in clean_results if i not in matches) <= 2:\n",
    "        metrics[\"List Acc @2\"] += 1\n",
    "\n",
    "assert total == (metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"])\n",
    "assert total == (metrics[\"p\"] + metrics[\"n\"])\n",
    "assert metrics[\"p\"] == (metrics[\"tp\"] + metrics[\"fn\"])\n",
    "assert metrics[\"n\"] == (metrics[\"tn\"] + metrics[\"fp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dc1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Word Level Metrics ====\n",
      "\n",
      "Precision:      0.934\n",
      "Recall:         0.958\n",
      "F1:             0.946\n",
      "Accuracy:       0.926\n",
      "--------------------\n",
      "Positives: 3394 Negatives: 1661 Total: 5055\n",
      "Positive Ratio: 0.671 Negative Ratio: 0.329\n",
      "\n",
      "==== List Level Metrics ====\n",
      "\n",
      "Accuracy:       0.739\n",
      "Accuracy @1:    0.949\n",
      "Accuracy @2:    0.984\n",
      "--------------------\n",
      "Total Lists: 1110\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(metrics):\n",
    "    total = metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"]\n",
    "    print(\"==== Word Level Metrics ====\")\n",
    "    print()\n",
    "    print(\"Precision:\".ljust(15), round(metrics[\"tp\"] / (metrics[\"tp\"] + metrics[\"fp\"]), 3))\n",
    "    print(\"Recall:\".ljust(15), round(metrics[\"tp\"] / (metrics[\"tp\"] + metrics[\"fn\"]), 3))\n",
    "    print(\"F1:\".ljust(15), round(2 * metrics[\"tp\"] / (2 * metrics[\"tp\"] + metrics[\"fp\"] + metrics[\"fn\"]), 3))\n",
    "    print(\"Accuracy:\".ljust(15), round((metrics[\"tp\"] + metrics[\"tn\"]) / (metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"]), 3))\n",
    "    print('-'*20)\n",
    "    print(\"Positives:\", metrics[\"p\"], \"Negatives:\", metrics[\"n\"], \"Total:\", total)\n",
    "    print(\"Positive Ratio:\", round(metrics[\"p\"] / total, 3), \"Negative Ratio:\", round(metrics[\"n\"] / total, 3))\n",
    "    print()\n",
    "    print(\"==== List Level Metrics ====\")\n",
    "    print()\n",
    "    print(\"Accuracy:\".ljust(15), round(metrics[\"List Acc\"] / len(ref_ds), 3))\n",
    "    print(\"Accuracy @1:\".ljust(15), round(metrics[\"List Acc @1\"] / len(ref_ds), 3))\n",
    "    print(\"Accuracy @2:\".ljust(15), round(metrics[\"List Acc @2\"] / len(ref_ds), 3))\n",
    "    print('-'*20)\n",
    "    print(\"Total Lists:\", len(ref_ds))\n",
    "\n",
    "\n",
    "\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08645040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(idx):\n",
    "    print(\"Full List:\".ljust(20), ref_ds[idx][\"full_list\"])\n",
    "    print(\"Cleaned List:\".ljust(20), ref_ds[idx][\"clean_list\"])\n",
    "    predicted_list = []\n",
    "    for i in get_results(ds, ref_ds, idx):\n",
    "        if i[\"label\"] != \"O\":\n",
    "            predicted_list.append(i[\"token\"])\n",
    "    print(\"Predicted List:\".ljust(20), predicted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59cc5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full List:           ['pensiero', 'sogno', 'incubo']\n",
      "Cleaned List:        ['pensiero', 'sogno', 'incubo']\n",
      "Predicted List:      ['pensiero', 'sogno', 'incubo']\n"
     ]
    }
   ],
   "source": [
    "print_results(111)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
