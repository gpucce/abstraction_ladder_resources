{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import load_from_disk, load_dataset, Dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ..utils import tokenize_and_align_labels\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fe6925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = \"with_spec\"\n",
    "experiment_name = \"without_spec\"\n",
    "# experiment_name = \"without_spec_t1_vs_t2\"\n",
    "\n",
    "if experiment_name == \"with_spec\":\n",
    "    dataset_name = \"./training_datasets/training_data_all_with_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec\":\n",
    "    dataset_name = \"./training_datasets/training_data_all_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec_t1_vs_t2\":\n",
    "    dataset_name = \"training_t1_test_t2_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"without_spec_t2_vs_t1\":\n",
    "    dataset_name = \"training_t2_test_t1_no_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"with_spec_t1_vs_t2\":\n",
    "    dataset_name = \"training_t1_test_t2_with_spec.ds\"\n",
    "\n",
    "elif experiment_name == \"with_spec_t2_vs_t1\":\n",
    "    dataset_name = \"training_t2_test_t1_with_spec.ds\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"experiment_name must be one of the predefined options.\")\n",
    "\n",
    "run_path = \"/home/gpucce/Repos/abstraction_ladders/acl_abstraction_ladders/primary_school_data/primary_school_experiments/multirun/2025-07-15/18-23-29\"\n",
    "model_name_or_path = None\n",
    "runs = os.listdir(run_path)\n",
    "for run in runs:\n",
    "    with open(os.path.join(run_path, run, \".hydra\", \"config.yaml\")) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    if config[\"args\"][\"dataset_name\"].split('/')[-1] == dataset_name.split('/')[-1]:\n",
    "        model_name_or_path = os.path.join(run_path, run)\n",
    "        break\n",
    "\n",
    "assert model_name_or_path is not None, f\"Model not found for dataset {dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "377d03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name_or_path,trust_remote_code=True,)\n",
    "model.to(\"cuda\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48173d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd93301c0524f168b5d47e7fb08a5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5943 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ds = load_from_disk(dataset_name)\n",
    "# ref_ds = load_from_disk(dataset_name)\n",
    "# if \"validation\" in ds:\n",
    "#     ds = ds[\"validation\"]\n",
    "#     ref_ds = ref_ds[\"validation\"]\n",
    "\n",
    "df = pd.read_csv(\"word_ladders_cleaned.csv\", sep=\"\\t\").loc[:, [\"start\", \"ladder\"]]\n",
    "df.ladder = df.ladder.apply(lambda x: ast.literal_eval(x))\n",
    "df.ladder = df.ladder.apply(lambda x: [l if l != \"entita'\" else \"entitÃ \" for l in x])  # Fixing the apostrophe issue\n",
    "df[\"full_list\"] = df[\"ladder\"]\n",
    "df[\"label\"] = df[\"full_list\"].apply(lambda x: [model.config.label2id.get(word, \"O\") for word in x])\n",
    "df[\"clean_list\"] = df[\"full_list\"]\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ref_ds = Dataset.from_pandas(df)\n",
    "\n",
    "ds = ds.map(lambda x: tokenize_and_align_labels(x, tokenizer, label_to_id=model.config.label2id), batched=True)\n",
    "new_ds = ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbe5505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(ds, ref_ds, idx, do_print=False):\n",
    "    results = []\n",
    "    out = model(**{i:j.to(\"cuda\") for i, j in ds[[idx]].items()})\n",
    "    preds = out.logits.cpu().argmax(-1)[0].tolist()[1:]\n",
    "    tokens = ref_ds[idx][\"full_list\"]\n",
    "\n",
    "    # tokenization check\n",
    "    refs = ds[idx][\"input_ids\"][ds[idx][\"attention_mask\"].bool()].tolist()\n",
    "    check = []\n",
    "    for tok in tokens:\n",
    "        check += tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    assert refs[1:-1] == check\n",
    "\n",
    "    count = 0\n",
    "    for tok in tokens:\n",
    "        tokenized = tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "        label = model.config.id2label[preds[count]]\n",
    "        count += len(tokenized)\n",
    "        results.append({\"token\": tok, \"label\": label,})\n",
    "        if do_print:\n",
    "            print(\"Token:\", tok, \"| Label:\", label)\n",
    "\n",
    "    return results  # Remove the first empty token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c11dab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510273e86ca74db28692ac394f800616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples_to_check = 500 # len(ref_ds)\n",
    "all_results = []\n",
    "for idx in tqdm(range(n_samples_to_check)):\n",
    "    result = get_results(ds, ref_ds, idx)\n",
    "    all_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb93a99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed90de54e654e8c8457b20a4ea77036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total = 0\n",
    "metrics = {\"n\": 0, \"p\": 0, \"tp\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"List Acc\": 0, \"List Acc @1\": 0, \"List Acc @2\": 0}\n",
    "for idx in tqdm(range(n_samples_to_check)):\n",
    "    results = all_results[idx]\n",
    "    full_list = ref_ds[idx][\"full_list\"]\n",
    "    clean_results = ref_ds[idx][\"clean_list\"]\n",
    "    total += len(full_list)\n",
    "    metrics[\"p\"] += len(clean_results)\n",
    "    metrics[\"n\"] += len(full_list) - len(clean_results)\n",
    "    metrics[\"tp\"] += sum(1 for r in results if r[\"label\"] != \"O\" and r[\"token\"] in clean_results)\n",
    "    metrics[\"tn\"] += sum(1 for r in results if r[\"label\"] == \"O\" and r[\"token\"] not in clean_results)\n",
    "    metrics[\"fn\"] += sum(1 for r in results if r[\"label\"] == \"O\" and r[\"token\"] in clean_results)\n",
    "    metrics[\"fp\"] += sum(1 for r in results if r[\"label\"] != \"O\" and r[\"token\"] not in clean_results)\n",
    "    matches = [i[\"token\"] for i in results if i[\"label\"] != \"O\"]\n",
    "    if matches == clean_results:\n",
    "        metrics[\"List Acc\"] += 1\n",
    "    if sum(1 for i in matches if i not in clean_results) + sum(1 for i in clean_results if i not in matches) <= 1:\n",
    "        metrics[\"List Acc @1\"] += 1\n",
    "    if sum(1 for i in matches if i not in clean_results) + sum(1 for i in clean_results if i not in matches) <= 2:\n",
    "        metrics[\"List Acc @2\"] += 1\n",
    "\n",
    "assert total == (metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"])\n",
    "assert total == (metrics[\"p\"] + metrics[\"n\"])\n",
    "assert metrics[\"p\"] == (metrics[\"tp\"] + metrics[\"fn\"])\n",
    "assert metrics[\"n\"] == (metrics[\"tn\"] + metrics[\"fp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6dc1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Word Level Metrics ====\n",
      "\n",
      "Precision:      1.0\n",
      "Recall:         0.233\n",
      "F1:             0.378\n",
      "Accuracy:       0.233\n",
      "--------------------\n",
      "Positives: 3734 Negatives: 0 Total: 3734\n",
      "Positive Ratio: 1.0 Negative Ratio: 0.0\n",
      "\n",
      "==== List Level Metrics ====\n",
      "\n",
      "Accuracy:       0.001\n",
      "Accuracy @1:    0.004\n",
      "Accuracy @2:    0.013\n",
      "--------------------\n",
      "Total Lists: 5943\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(metrics):\n",
    "    total = metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"]\n",
    "    print(\"==== Word Level Metrics ====\")\n",
    "    print()\n",
    "    print(\"Precision:\".ljust(15), round(metrics[\"tp\"] / (metrics[\"tp\"] + metrics[\"fp\"]), 3))\n",
    "    print(\"Recall:\".ljust(15), round(metrics[\"tp\"] / (metrics[\"tp\"] + metrics[\"fn\"]), 3))\n",
    "    print(\"F1:\".ljust(15), round(2 * metrics[\"tp\"] / (2 * metrics[\"tp\"] + metrics[\"fp\"] + metrics[\"fn\"]), 3))\n",
    "    print(\"Accuracy:\".ljust(15), round((metrics[\"tp\"] + metrics[\"tn\"]) / (metrics[\"tp\"] + metrics[\"tn\"] + metrics[\"fp\"] + metrics[\"fn\"]), 3))\n",
    "    print('-'*20)\n",
    "    print(\"Positives:\", metrics[\"p\"], \"Negatives:\", metrics[\"n\"], \"Total:\", total)\n",
    "    print(\"Positive Ratio:\", round(metrics[\"p\"] / total, 3), \"Negative Ratio:\", round(metrics[\"n\"] / total, 3))\n",
    "    print()\n",
    "    print(\"==== List Level Metrics ====\")\n",
    "    print()\n",
    "    print(\"Accuracy:\".ljust(15), round(metrics[\"List Acc\"] / len(ref_ds), 3))\n",
    "    print(\"Accuracy @1:\".ljust(15), round(metrics[\"List Acc @1\"] / len(ref_ds), 3))\n",
    "    print(\"Accuracy @2:\".ljust(15), round(metrics[\"List Acc @2\"] / len(ref_ds), 3))\n",
    "    print('-'*20)\n",
    "    print(\"Total Lists:\", len(ref_ds))\n",
    "\n",
    "\n",
    "\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08645040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(idx):\n",
    "    print(\"Full List:\".ljust(20), ref_ds[idx][\"full_list\"])\n",
    "    # print(\"Cleaned List:\".ljust(20), ref_ds[idx][\"clean_list\"])\n",
    "    predicted_list = []\n",
    "    for i in get_results(ds, ref_ds, idx):\n",
    "        if i[\"label\"] != \"O\":\n",
    "            predicted_list.append(i[\"token\"])\n",
    "    print(\"Predicted List:\".ljust(20), predicted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "59cc5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full List:           ['fox', 'volpe', 'mammifero']\n",
      "Predicted List:      []\n"
     ]
    }
   ],
   "source": [
    "print_results(269)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
